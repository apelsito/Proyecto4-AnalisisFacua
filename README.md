# üõí An√°lisis de Precios de Supermercados üìä

Este proyecto tiene como objetivo recopilar, procesar y analizar datos de precios de productos en distintos supermercados de Espa√±a, con el fin de comparar precios, estudiar la evoluci√≥n de precios a lo largo del tiempo y detectar posibles anomal√≠as. La informaci√≥n recolectada permite visualizar la dispersi√≥n de precios entre supermercados y ofrece una base de datos s√≥lida para futuras investigaciones de tendencias de precios.

# Descripci√≥n del Proyecto üí°

Este proyecto tiene como objetivo analizar los precios de productos en distintos supermercados en Espa√±a, utilizando datos extra√≠dos de la p√°gina [**FACUA: Precios Supermercados**](https://super.facua.org/) mediante t√©cnicas de web scraping. Los datos recolectados se almacenan en una base de datos SQL y se analizan con Python y Pandas. 

## Principales Componentes del Proyecto üîç

- **Comparaci√≥n de precios**: Evaluamos las diferencias de precios entre supermercados para identificar las opciones m√°s competitivas en el mercado.
- **Evoluci√≥n de precios**: Analizamos las tendencias de los precios de distintos productos a lo largo del tiempo para entender su comportamiento y fluctuaciones.
- **Detecci√≥n de anomal√≠as**: Identificamos variaciones de precios inusuales para se√±alar anomal√≠as o patrones inesperados en el mercado.
- **Visualizaci√≥n de datos**: Generamos gr√°ficos interactivos con Plotly Express, facilitando la interpretaci√≥n de los datos y permitiendo una exploraci√≥n visual din√°mica de los resultados.

Este an√°lisis proporciona una visi√≥n detallada de c√≥mo var√≠an los precios en el mercado, ayudando a identificar patrones y realizar comparaciones √∫tiles para el consumidor.

# Estructura del Proyecto üóÇÔ∏è

```bash
Proyecto4-AnalisisFacua/
‚îú‚îÄ‚îÄ datos/                   # Tablas con la informaci√≥n de SQL.
‚îÇ   ‚îú‚îÄ‚îÄ 01_backups/          # Backups de webscraping para no tener que rehacerlos.
‚îÇ   ‚îú‚îÄ‚îÄ 02_consultas_sql/    # Tablas de las consultas realizadas en SQL.
‚îÇ
‚îú‚îÄ‚îÄ notebooks/               # Notebooks de jupyter con el an√°lisis y las visualizaciones.
‚îú‚îÄ‚îÄ src/                     # Archivos .py para las funciones utilizadas por el proyecto
‚îÇ    ‚îú‚îÄ‚îÄ 01_png/             # Fotos de evidencias sobre hallazgos.
‚îÇ    ‚îú‚îÄ‚îÄ 02_graficas/        
‚îÇ    ‚îÇ    ‚îú‚îÄ‚îÄ html/          # Gr√°ficas interactivas en HTML.  
‚îÇ    ‚îÇ    ‚îú‚îÄ‚îÄ png/           # Fotos de las mismas gr√°ficas.
‚îÇ
‚îî‚îÄ‚îÄ README.md                # Descripci√≥n del proyecto, lo est√°s leyendo!
```


# Instalaci√≥n y Requisitos üõ†Ô∏è
## Requisitos

Para ejecutar este proyecto, aseg√∫rate de tener instalado lo siguiente:

- **Python 3.x** üêç
- **Jupyter Notebook** üìì para ejecutar y visualizar los an√°lisis de datos
- **Bibliotecas de Python**:
    - [pandas](https://pandas.pydata.org/docs/) para manipulaci√≥n de datos üßπ
    - [numpy](https://numpy.org/doc/2.1/) para c√°lculos num√©ricos üî¢
    - [plotly.express](https://plotly.com/python/plotly-express/) para visualizaci√≥n de datos interactiva üìä
    - [requests](https://requests.readthedocs.io/en/latest/) para conectar con FACUA y realizar peticiones HTTP üåê
    - [selenium](https://www.selenium.dev/documentation/) para interactuar con sitios web din√°micos üíª
    - [beautifulsoup4](https://beautiful-soup-4.readthedocs.io/en/latest/) para scraping de sitios web üï∏Ô∏è
    - [dotenv](https://www.dotenv.org/docs/) para manejar variables de entorno de manera segura üîê
    - [tqdm](https://tqdm.github.io/) para crear barras de progreso en los procesos largos ‚è≥
    - [psycopg2](https://www.psycopg.org/docs/) para conectarse y ejecutar consultas en PostgreSQL üõ¢Ô∏è
- **Para crear la Base de Datos**
    - [PostgreSQL](https://www.postgresql.org/) para la gesti√≥n y almacenamiento de datos relacionales üìÇ
    - [DBeaver](https://dbeaver.io/) para administraci√≥n y consulta visual de bases de datos üñ•Ô∏è

## Instalaci√≥n üõ†Ô∏è

1. Clona este repositorio para visualizarlo en vscode:
```bash
git clone https://github.com/apelsito/Proyecto4-AnalisisFacua.git
cd Proyecto4-AnalisisFacua
```
# Desarrollo del Proyecto üöÄ

Este proyecto se ha desarrollado en varias fases para asegurar una recopilaci√≥n y procesamiento de datos estructurados, seguido de su an√°lisis. A continuaci√≥n, se describen las fases clave:

## Fase 1: Scraping üîç

La primera fase del proyecto consiste en recolectar datos de precios de productos mediante web scraping en la p√°gina de FACUA.

### Pasos:
1. **Obtener URLs de Supermercados**: Usando Selenium, recopilamos todas las URLs de los supermercados disponibles en la p√°gina.
2. **Extraer URLs de Productos**: A partir de las URLs de los supermercados, extraemos las URLs de las categor√≠as clave:
    - **Aceite de girasol**
    - **Aceite de oliva**
    - **Leche**
3. **Obtener URLs de Subcategor√≠as**: Nos adentramos en subcategor√≠as espec√≠ficas para mayor precisi√≥n en la extracci√≥n de datos:
    - **Aceite de oliva**: Suave e Intenso, Virgen, Virgen Extra.
    - **Leche**: Enriquecida, Entera/Semi/Desnatada, Sin Lactosa.

   Por cada supermercado, nos quedamos con las siguientes URLs espec√≠ficas:
   - URL Aceite Girasol
   - URL Aceite de Oliva Suave e Intenso
   - URL Aceite de Oliva Virgen
   - URL Aceite de Oliva Virgen Extra
   - URL Leche Enriquecida
   - URL Leche Entera, Semi o Desnatada
   - URL Leche Sin Lactosa

4. **Extraer Historicos de Productos**: Con BeautifulSoup, extraemos las URLs del historial de cada producto para su an√°lisis posterior.

### Observaciones del web-scraping üìå

Al revisar los datos en busca de duplicados, observamos que algunos productos aparecen listados dos veces en la web. Por ejemplo:

![Producto duplicado](src/01_png/01_Producto_duplicado.png)

Al examinar los historiales de ambos productos duplicados, encontramos que la URL de cada uno es id√©ntica:

![Hist√≥rico con URL duplicada](src/01_png/02_URL_es_lo_mismo.png)
![Hist√≥rico con URL duplicada](src/01_png/02_URL_es_lo_mismo.png)

Esta duplicaci√≥n en los listados ocurre de forma recurrente. Por ello, hemos decidido eliminar los duplicados sin riesgo de perder informaci√≥n, ya que los datos de cada producto duplicado se refieren al mismo √≠tem en la base de datos de Facua.

## Fase 2: Preparaci√≥n Pre DB üóÑÔ∏è

Antes de almacenar los datos en la base de datos, preparamos los DataFrames necesarios.

### Pasos:
1. **Separaci√≥n de Tablas**: Organizamos los datos en cuatro tablas para la base de datos:
   - **Supermercado**
   - **Categor√≠a**
   - **Producto**
   - **Hist√≥rico**
   
2. **Crear DataFrames**: Creamos DataFrames individuales para cada entidad con dos columnas:
   - Un √≠ndice √∫nico (iniciando en 1) para compatibilidad con SQL.
   - Una columna con los valores √∫nicos de cada entidad.

3. **Generaci√≥n de Claves For√°neas**: En el DataFrame de **Hist√≥rico**, generamos un diccionario de cada tabla, reemplazando los valores de Supermercado, Categor√≠a y Producto por sus √≠ndices correspondientes. Esto permite establecer relaciones entre tablas en la base de datos mediante claves for√°neas.

## Fase 3: Creaci√≥n de la Base de Datos üõ¢Ô∏è

Con los DataFrames preparados, procedemos a crear la base de datos en PostgreSQL.

### Pasos:
1. **Configuraci√≥n de la Base de Datos en DBeaver**:
   - Abrimos PostgreSQL en DBeaver y creamos una nueva base de datos llamada `historicos` con la siguiente configuraci√≥n:

   ![Creaci√≥n de Base de Datos en DBeaver](src/01_png/03_DBeaver.png)
   ![Ajustes de la Base de Datos](src/01_png/04_DBeaver_newDB.png)

2. **Crear las Tablas**:
   - **Supermercados**:
     ```sql
     CREATE TABLE supermercados (
       id_supermercado SERIAL PRIMARY KEY,
       supermercado VARCHAR (100) NOT NULL
     );
     ```
   - **Categor√≠as**:
     ```sql
     CREATE TABLE categorias (
       id_categoria SERIAL PRIMARY KEY,
       categoria VARCHAR (100) NOT NULL
     );
     ```
   - **Productos**:
     ```sql
     CREATE TABLE productos (
       id_producto SERIAL PRIMARY KEY,
       producto VARCHAR (1000) NOT NULL
     );
     ```
   - **Hist√≥ricos**:
     ```sql
     CREATE TABLE historicos (
       id_supermercado INT NOT NULL,
       fecha DATE,
       id_producto INT NOT NULL,
       id_categoria INT NOT NULL,
       variacion_euros DECIMAL(5,2) NOT NULL,
       variacion_porcentaje DECIMAL(5,2) NOT NULL,
       FOREIGN KEY (id_producto) REFERENCES productos (id_producto) ON UPDATE CASCADE ON DELETE RESTRICT,
       FOREIGN KEY (id_categoria) REFERENCES categorias (id_categoria) ON UPDATE CASCADE ON DELETE RESTRICT,
       FOREIGN KEY (id_supermercado) REFERENCES supermercados (id_supermercado) ON UPDATE CASCADE ON DELETE RESTRICT
     );
     ```

Estas tablas estructuran los datos de manera que se puedan realizar consultas eficientes y mantener la integridad de las relaciones entre los datos.

# Visualizaci√≥n y An√°lisis de Gr√°ficas üìä

Para profundizar en los datos recolectados, hemos realizado diversas consultas SQL que nos permiten extraer informaci√≥n clave sobre precios, variaciones y patrones de los productos en los supermercados. Estas consultas nos facilitan analizar la dispersi√≥n de precios, identificar tendencias y detectar anomal√≠as. A continuaci√≥n, presentaremos una serie de gr√°ficas interactivas basadas en estos datos, que ofrecen una visi√≥n clara y detallada del comportamiento de los precios en el mercado.


## Conclusiones ‚úàÔ∏è



# Contribuciones ü§ù

Las contribuciones a este proyecto son muy bienvenidas. Si tienes alguna sugerencia, mejora o correcci√≥n, no dudes en ponerte en contacto o enviar tus ideas.

Cualquier tipo de contribuci√≥n, ya sea en c√≥digo, documentaci√≥n o feedback, ser√° valorada. ¬°Gracias por tu ayuda y colaboraci√≥n!

# Autores y Agradecimientos ‚úçÔ∏è

## Autor ‚úíÔ∏è
**Gonzalo Ruip√©rez Ojea** - [@apelsito](https://github.com/apelsito) en github

## Agradecimientos
Quiero expresar mi agradecimiento a **Hackio** y su equipo por brindarme la capacidad y las herramientas necesarias para realizar este proyecto con solo una semana de formaci√≥n. Su apoyo ha sido clave para lograr este trabajo.

